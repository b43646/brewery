= Overview

**Green Cloud Demo** is your first step on how to migrate and optimize an existing Spring Boot application  to
https://www.openshift.com[OpenShift].  The migration guides you with the process of how to migrate Spring Boot workload form other platform 
to https://www.openshift.com[OpenShift], i.e. the build process, the ideal platform (https://www.openshift.com[OpenShift]), optimizing etc.,


== Short History of Microservices

.History of Microservices
image::./History_Of_Microservices.png[title=Short History of Microservices,align=center]

Most of the https://netflix.github.io/[Netflix OSS] components listed above are pretty old and optimized more for AWS, they are prone to some common DevOps
pain-points for organizations that are starting to adopt DevOps. The migration that will be done as part of this demo will help in alleviating those possible
DevOps pain-points and provide the organizations a direction on **How to migrate my Spring Boot Application to OpenShift**

== App Overview 
In this demo, the https://github.com/kameshsampath/brewery[Spring Cloud Samples - Brewery] will be migrated 
and optimized for OpenShift, during the process of migration the original https://github.com/spring-cloud-samples/brewery[Spring Cloud Samples - Brewery]
will be modified to make it deployable on to https://kubernetes.io[Kubernetes] or https://www.openshift.com[Openshift].

The application will be migrated iteratively,

* [*] https://redhat-developer-docs.github.io/green-cloud-demo/#iteration-1[Iteration-I] - As-is deployment of the https://github.com/spring-cloud-samples/brewery[Spring Cloud Samples - Brewery]
with no code change.  The Application build process will be modified to enable easier deployment of application on Openshift

* [*] <<iteration-2>> - Will use native OpenShift/Kubernetes features such as service discovery, loadbalancing & externalization of the config

* [*] https://redhat-developer-docs.github.io/green-cloud-demo/#iteration-3[Iteration-III], the current iteration - Optimizing stacks on OpenShift, like Apache Artemis instead of RabbitMQ, using OpenTracing and Jaeger

[[default-pre-req]]
== Pre-Requisite

You have a OpenShift cluster running locally using https://docs.openshift.org/latest/minishift/getting-started/index.html[*minishift*]
or https://developers.redhat.com/products/cdk/overview/Op[*CDK*], or
have access to https://www.openshift.com/container-platform/index.html[*OpenShift Container Platform*]

Check the https://redhat-developer-docs.github.io/green-cloud-demo/#res-tools[Tools] section for more details

[WARNING]
====
- At least 7Gb of RAM is required to run the Brewery application, atleast for https://redhat-developer-docs.github.io/green-cloud-demo/#iteration-1[Iteration-I]
====

== Docker Setup

Before doing any deployment, its recommended to do `eval $(minishift docker-env)` from your current shell, to set up the DOCKER environment variables, that will be needed by the fabric8 maven plugin to deploy application on OpenShift.

[[iteration-2]]
= Iteration II

The Iteration II will <<itr2-deprecated-modules,deprecate>> few of the https://netflix.github.io/[NetFlix OSS] components that are superflous
inside https://kubernetes.io[Kubernetes] or https://www.openshift.com[Openshift]. The following sections shows how to get
the Iteration II deployed on to https://kubernetes.io[Kubernetes] or https://www.openshift.com[Openshift]. This iteration
uses the https://github.com/spring-cloud-incubator/spring-cloud-kubernetes[Spring Cloud Kubernetes] -
the http://projects.spring.io/spring-cloud/[Spring Cloud] based discovery client for Kubernetes

[[itr2-application-setup]]
== Setup

[[itr2-clone-source]]
=== Clone

[source,sh]
----
git clone -b iteration-2 https://github.com/redhat-developer-demos/brewery.git
----

[NOTE]
====
Through out this document we will call the directory where the project was cloned as _$PROJECT_HOME_
====


[[itr2-pre-req]]
== Pre-Requisite

[[itr2-pre-req-general]]
=== General
The spring-cloud-kubernetes library used in the project requires the `default` service account to have view permissions, to enable that we execute the following command,

[source,sh]
----
oc policy add-role-to-user view -z default -n $(oc project -q)
----

[TIP]
====
The Service Account `default` does not have any permission, in order to allow `default` SA to lookup the ConfigMaps within the namespace, it need to be added with `view`
role 
====

[[itr2-pre-req-rabbitmq]]
=== RabbitMQ 

The https://hub.docker.com/_/rabbitmq/[RabbimtMQ] container image when run is run using `root` user (UID `0`).  The OpenShift by default does not allow container 
images to be run with UID `0`. In order to allow that we need to define polices and attach to Kubernetes Service Account, to have better clarity and control 
we create a separate service account called **brewery** and add the required https://docs.openshift.org/latest/admin_guide/manage_scc.html[Security Context Constraints(SCC)] 
to allow running the container as UID `0` user.  The following commands adds the required SCC to the service account **brewery**,

[source,sh]
----
oc adm policy add-scc-to-user privileged -z brewery <1>

oc adm policy add-scc-to-user anyuid -z brewery  <1>
----

<1> **brewery** service is created when RabbitMQ is deployed

[[itr2-deployable-apps]]
== Deploy Applications

.Application List
[cols="1*^,1,1,5"]
|===
| |Application| Folder | Remarks

|icon:check[color: green]
|<<rabbitmq>>
|*$PROJECT_HOME*/extras/rabbitmq
|Message Broker - https://www.rabbitmq.com/

|icon:check[color: green]
|common
|*$PROJECT_HOME*/common
|Common shared library

|icon:check[color: green]
|common-zipkin-stream
|*$PROJECT_HOME*/ommon-zipkin-stream
|Common shared library for the projects that uses the Sleuth Zipkin Stream for tracing

|[red]#*X*#
|[red]#eureka#
|[red]#*$PROJECT_HOME*/eureka#
|Application will use https://kubernetes.io/docs/concepts/services-networking/service/[Kubernetes Services]

|[red]#*X*#
|[red]#config-server#
|[red]#*$PROJECT_HOME*/config-server#
|Application will use https://kubernetes.io/docs/tasks/configure-pod-container/configmap/[Kubernetes ConfigMaps]

|icon:check[color: green]
|<<itr2-zipkin-server>>
|*$PROJECT_HOME*/zipkin-server
| http://zipkin.io/[Distributed Tracing system]

|icon:check[color: green]
|<<itr2-zuul>>
|*$PROJECT_HOME*/zuul
| https://github.com/Netflix/zuul/wiki[Java based Proxy]

|icon:check[color: green]
|<<itr2-ingredients>>
|*$PROJECT_HOME*/ingredients
|

|icon:check[color: green]
|<<itr2-reporting>>
|*$PROJECT_HOME*/reporting
|

|icon:check[color: green]
|<<itr2-brewing>>
|*$PROJECT_HOME*/brewing
|

|icon:check[color: green]
|<<itr2-presenting>>
|*$PROJECT_HOME*/presenting
|

|===

[[itr2-build-app]]
=== Building

The Iteration II of the brewery application has migrated all the projects to http://maven.apache.org/[Apache Maven] based build.

To build the application run the following command:

[source,sh]
----
./mvnw -N install <1>
./mvnw clean install <2>
----
<1> This will install the brewery parent pom in local maven repository
<2> This will build the applications, if the minishift or OpenShift cluster is running, this will trigger `s2i` builds
of the respective application as well

[[itr2-deploy-to-openshift]]
=== Deploying to OpenShift

The following section details on how to deploy the Iteration II to OpenShift.

[IMPORTANT]
====
Ensure that all <<itr2-pre-req,Pre-Requisite>> are done before starting deployment.
====

[[itr2-rabbitmq]]
==== RabbitMQ

[[itr2-rabbitmq-local]]
===== Local Deployment

Go to the directory  *$PROJECT_HOME/extras/rabbitmq*, and execute the following command

[source,sh]
----
./mvnw -Dfabric8.mode=kubernetes clean fabric8:deploy
----

[[itr2-rabbitmq-cloud]]
===== External Cloud Deployment

Sometimes you might have access to docker socket typical case when deploying to external cloud, in those cases you can run the following set of commands,

[source,sh]
----
./mvnw clean fabric8:resource
oc apply -f target/classes/META-INF/fabric8/openshift.yml
----

This will take some time to get it running as the deployment needs to download the `rabbitmq` docker image

[[itr2-zipkin-server]]
==== Zipkin Server

Go to the directory  *$PROJECT_HOME/zipkin-server*, and execute the following command

[source,sh]
----
./mvnw fabric8:deploy
----

[[itr2-zuul]]
==== Zuul

Go to the directory  *$PROJECT_HOME/zuul*, and execute the following command

[source,sh]
----
./mvnw fabric8:deploy
----

[[itr2-ingredients]]
==== Ingredients

Go to the directory  *$PROJECT_HOME/ingredients*, and execute the following command

[source,sh]
----
./mvnw fabric8:deploy
----

[[itr2-reporting]]
==== Reporting

Go to the directory  *$PROJECT_HOME/reporting*, and execute the following command

[source,sh]
----
./mvnw fabric8:deploy
----

[[itr2-brewing]]
==== Brewing

Go to the directory  *$PROJECT_HOME/brewing*, and execute the following command

[source,sh]
----
./mvnw fabric8:deploy
----

[[itr2-presenting]]
==== Presenting

Go to the directory  *$PROJECT_HOME/presenting*, and execute the following command

[source,sh]
----
./mvnw fabric8:deploy
----

[[itr2-acceptance-testing]]
== Acceptance Testing

The *$PROJECT_HOME/acceptance-tests* holds the test cases for testing the application.  To perform
we need to have have some ports forwarded from Kubernetes/OpenShift to localhost(where you build the application)

[source,sh]
----
oc port-forward zipkin-1-06wmt 9411:8080 <1>
oc port-forward presenting-1-wzhfn 9991:8080 <2>
----

<1> forward port 8080 from Zipkin pod to listen on localhost:9411
<2> forward port 8080 from Presenting pod to listen on localhost:9991

NOTE: Please update the pod names based on your local deployment

To run acceptance testing, execute following command from $PROJECT_HOME,

[source,sh]
----
 ./mvnw clean test
----

[[itr2-deprecated-modules]]

== Deprecated Modules

As part of Iteration-II the following modules have been deprecated,

* Eureka
* Config Server
* common-zipkin
* common-zipkin-old
* zookeeper
* docker
